# 线性回归
`举个例子`
先考虑房屋平均面积与多个参数的关系

补充参数($x_0$)|居住面积($x_1$)|卧室数量($x_2$)|...|每平米价格($y$)
----|-----|------|----|------
1|2000|3|...|400
1|1950|2|...|350
...|...|...|...|...

假设每行为每个样本，则每一行可以表示为
$$
h_\theta(x) = \theta_0+\theta_1 x_1+...+\theta_n x_n
$$
现在为每条样本补充一个值为1的参数$x_0$，并将$\theta$和$x$写成列向量，则
$$
h_\theta(x) = {\vec{\theta}} ^T \vec{x}
$$
这是一个参数$\theta$不定的关于$x$的方程，因为每条样本数据不可能同时对于一个$\vec{\theta}$都成立，所以我们要在其向量乘积后面添上一个偏移量$\epsilon ^{i}$表示第$i$个样本套用$\theta$参数后计算出的值与样本真实值的偏差
所以对第$i$条样本来说
$$
y^i = h_\theta(x^i) + \epsilon^i
$$
若参数特征选择合理，则$\epsilon ^i \sim N(0,\sigma ^2)$，即期望为0的正态分布

## 求解最佳$\theta$
为了使样本数据最高程度的拟合方程，则我们以$\theta$为参数的方程
$$
J(\theta) = \sum_{i=1}^n (\epsilon^i)^2
$$
使其值最小，即方程
$$
J(\theta) = \sum_{i=1}^n (h_\theta(x^i) - y^i)^2
$$
的值最小
记$m$个$n$维样本组成的矩阵为$X_{m*n}$，样本结果记作向量$\vec{y}$，若$X^TX$可逆，则
$$
\frac{\partial J(\theta)}{\partial \theta} = X^TX\theta-X^T\vec{y} = 0
\\\\
\theta = (X^TX)^{-1}X^T\vec{y}
$$
即可求出$\theta$ 

### 过拟合
若模型能很好的拟合训练数据而不能正确的预测测试数据，则称其为过拟合，也就是将训练数据中一些其他非正常因素造成的误差也就是噪声也学习进去了
若$X^TX$不可逆或防止过拟合，我们可以增加$\lambda$扰动
$$
\theta = (X^TX + \lambda I)^{-1}X^T\vec{y}
$$

## 梯度下降
### 梯度概念
梯度跟导数概念类似，不同的是，导数是对一个数求导，梯度是对一个向量求导
如果了解方向导数的话，发现梯度跟方向导数是有关的，即梯度就是沿着向量$\theta$方向的方向导数，也就是$J(\theta)$变化最大的值

### 梯度下降法
同样以$\theta$为参数的方程
$$
J(\theta) = \sum_{i=1}^n (h_\theta(x^i) - y^i)^2
$$
先初始化向量$\theta$，在沿着负梯度方向迭代，更新后的$\theta$使得$J(\theta)$更小，即
$$
\theta = \theta - \alpha\frac{\partial J(\theta)}{\partial \theta}
$$
其中$\alpha$为学习率、步长

